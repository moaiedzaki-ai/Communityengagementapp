{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KamP67nRt-Ko"
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaRn1N6ZuHHg"
      },
      "source": [
        "#**Lab: Pytorch Basics**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZXwf7gKK_Ia"
      },
      "source": [
        "## **What is Pytorch?** <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/PyTorch_logo_icon.svg/500px-PyTorch_logo_icon.svg.png\" width=\"4%\">\n",
        "\n",
        "\n",
        "**PyTorch** is an open-source deep learning framework that allows us to build and train neural networks using **tensors** and **automatic differentiation**.  \n",
        "It provides simple, flexible tools to define models, compute gradients using backpropagation, and optimize parameters efficiently.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMLes6srWwMe"
      },
      "source": [
        "## \ud83d\udce6 **Tensors in PyTorch**\n",
        "\n",
        "A **tensor** is the main data structure in PyTorch.  \n",
        "It is similar to a NumPy array, but can run on both CPUs and GPUs.\n",
        "\n",
        "Tensors are used to represent: input data, model parameters, and model outputs\n",
        "\n",
        "### \ud83d\udd39 Creating Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_R5-1UJaw0w",
        "outputId": "6bc5de87-bc36-4580-fcc1-06b2d6eb71fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor([1., 2., 3.])\n",
            "y: tensor([0.3548, 0.3962, 0.4785])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create tensors\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "y = torch.randn(3)\n",
        "print(\"x:\", x)\n",
        "print(\"y:\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n**\ud83d\udca1 Explanation:**\n\nImports PyTorch and creates your first tensor!\n\n**What is a Tensor?**\n- PyTorch's version of a NumPy array\n- Can run on GPU for massive speedup\n- Supports automatic differentiation (for deep learning)\n\n**Creating tensors:**\n```python\nimport torch\n\n# From a Python list\nx = torch.tensor([1, 2, 3])\n\n# Create special tensors\nzeros = torch.zeros(3, 3)    # 3x3 matrix of zeros\nones = torch.ones(2, 4)      # 2x4 matrix of ones\nrandom = torch.randn(2, 3)   # Random numbers (normal distribution)\n```\n\n**Why Tensors vs NumPy Arrays?**\n\n**NumPy:**\n```python\nimport numpy as np\narr = np.array([1, 2, 3])\n# \u2717 Only runs on CPU\n# \u2717 No automatic differentiation\n# \u2713 Great for general computing\n```\n\n**PyTorch Tensors:**\n```python\nimport torch\ntensor = torch.tensor([1, 2, 3])\n# \u2713 Can run on GPU (100x faster!)\n# \u2713 Automatic gradients for deep learning\n# \u2713 Seamless deep learning integration\n```\n\n**Real-world analogy:**\n- NumPy array = Calculator (CPU only, basic operations)\n- PyTorch tensor = Supercomputer (CPU/GPU, advanced ML operations)\n\n**Like:** Using Excel vs using a specialized data science platform!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1boO-_nua7dB"
      },
      "source": [
        "### \ud83d\udd39 Tensor Shapes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BDn9esxa_o_",
        "outputId": "4a5e910b-0e78-4ec2-e013-968504777d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of x: torch.Size([3])\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of x:\", x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n**\ud83d\udca1 Explanation:**\n\nUnderstanding tensor shapes is crucial - they tell you the dimensions of your data!\n\n**Tensor dimensions explained:**\n\n**0D Tensor (Scalar):**\n```python\nx = torch.tensor(5)\nprint(x.shape)  # torch.Size([])\n# Just a single number\n```\n\n**1D Tensor (Vector):**\n```python\nx = torch.tensor([1, 2, 3, 4])\nprint(x.shape)  # torch.Size([4])\n# 4 elements in one dimension\n# Like a list of numbers\n```\n\n**2D Tensor (Matrix):**\n```python\nx = torch.tensor([[1, 2, 3],\n                  [4, 5, 6]])\nprint(x.shape)  # torch.Size([2, 3])\n# 2 rows, 3 columns\n# Like a spreadsheet\n```\n\n**3D Tensor:**\n```python\nx = torch.randn(2, 3, 4)\nprint(x.shape)  # torch.Size([2, 3, 4])\n# 2 matrices, each 3x4\n# Like multiple spreadsheets stacked\n```\n\n**4D Tensor (Common in images):**\n```python\nimages = torch.randn(32, 3, 224, 224)\nprint(images.shape)  # torch.Size([32, 3, 224, 224])\n# 32 images\n# 3 color channels (RGB)\n# 224x224 pixels each\n```\n\n**Reading shapes:**\n```\ntorch.Size([32, 3, 224, 224])\n             \u2191   \u2191   \u2191    \u2191\n          batch  channels height width\n```\n\n**Real-world examples:**\n\n**1D - Time series:**\n```python\nstock_prices = torch.tensor([100, 102, 98, 105, 103])\n# Shape: [5]\n# 5 days of prices\n```\n\n**2D - Tabular data:**\n```python\nstudents = torch.tensor([[85, 90, 78],  # Student 1: Math, Science, English\n                         [92, 88, 95],  # Student 2\n                         [78, 85, 82]]) # Student 3\n# Shape: [3, 3]\n# 3 students, 3 subjects\n```\n\n**3D - Video frames:**\n```python\nvideo = torch.randn(100, 720, 1280)\n# Shape: [100, 720, 1280]\n# 100 frames, each 720x1280 pixels\n```\n\n**4D - Batch of RGB images:**\n```python\nbatch = torch.randn(16, 3, 256, 256)\n# Shape: [16, 3, 256, 256]\n# 16 images, RGB (3 channels), 256x256 each\n```\n\n**Why shapes matter:**\n\n**Matrix multiplication requirements:**\n```python\nA = torch.randn(3, 4)  # [3, 4]\nB = torch.randn(4, 5)  # [4, 5]\nC = A @ B              # [3, 5] \u2713 Works!\n\n# But this fails:\nD = torch.randn(3, 4)\nE = torch.randn(3, 5)\nF = D @ E  # \u2717 Error! Shapes don't match\n```\n\n**Neural network layers:**\n```python\n# Input layer expects specific shape\ninput_tensor = torch.randn(32, 10)  # 32 samples, 10 features\nlayer = nn.Linear(10, 5)             # Expects input size 10\noutput = layer(input_tensor)         # Output: [32, 5] \u2713\n\nwrong_input = torch.randn(32, 8)     # Wrong feature count\noutput = layer(wrong_input)          # \u2717 Error!\n```\n\n**Common shape operations:**\n```python\nx = torch.randn(2, 3, 4)\n\n# Get number of dimensions\nx.ndim  # 3\n\n# Get size of specific dimension\nx.size(0)  # 2\nx.size(1)  # 3\nx.size(2)  # 4\n\n# Total number of elements\nx.numel()  # 2 \u00d7 3 \u00d7 4 = 24\n```\n\n**Like:**\n- 1D shape = Row in Excel\n- 2D shape = Excel spreadsheet\n- 3D shape = Multiple Excel sheets\n- 4D shape = Multiple Excel workbooks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfrk3ELobLIP"
      },
      "source": [
        "### \ud83d\udd39 Tensor Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Krsxjc3bYqo",
        "outputId": "93e63ace-c817-4889-e52e-50e2b45f9f50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Addition: tensor([5., 7., 9.])\n",
            "Multiplication: tensor([ 4., 10., 18.])\n",
            "Matrix multiplication result shape: torch.Size([2, 2])\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "# Element-wise operations\n",
        "print(\"Addition:\", a + b)\n",
        "print(\"Multiplication:\", a * b)\n",
        "\n",
        "# Matrix multiplication\n",
        "A = torch.randn(2, 3)\n",
        "B = torch.randn(3, 2)\n",
        "C = A @ B\n",
        "\n",
        "print(\"Matrix multiplication result shape:\", C.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n**\ud83d\udca1 Explanation:**\n\nPyTorch tensors support all standard mathematical operations!\n\n**Basic arithmetic:**\n```python\na = torch.tensor([1.0, 2.0, 3.0])\nb = torch.tensor([4.0, 5.0, 6.0])\n\n# Addition\nc = a + b  # [5.0, 7.0, 9.0]\n\n# Subtraction\nd = b - a  # [3.0, 3.0, 3.0]\n\n# Multiplication (element-wise)\ne = a * b  # [4.0, 10.0, 18.0]\n\n# Division\nf = b / a  # [4.0, 2.5, 2.0]\n```\n\n**Matrix operations:**\n```python\nA = torch.randn(3, 4)\nB = torch.randn(4, 5)\n\n# Matrix multiplication\nC = torch.matmul(A, B)  # [3, 5]\n# Or using @ operator\nC = A @ B  # Same as above\n```\n\n**Aggregation operations:**\n```python\nx = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n\nx.sum()   # 15.0 (sum of all elements)\nx.mean()  # 3.0 (average)\nx.std()   # 1.58 (standard deviation)\nx.max()   # 5.0 (maximum value)\nx.min()   # 1.0 (minimum value)\n```\n\n**Operations along dimensions:**\n```python\nmatrix = torch.tensor([[1, 2, 3],\n                       [4, 5, 6]])\n\n# Sum along rows (dim=0)\nmatrix.sum(dim=0)  # [5, 7, 9]\n#                      \u2191  \u2191  \u2191\n#                    1+4 2+5 3+6\n\n# Sum along columns (dim=1)\nmatrix.sum(dim=1)  # [6, 15]\n#                     \u2191   \u2191\n#                  1+2+3 4+5+6\n```\n\n**Broadcasting (automatic shape matching):**\n```python\n# Add scalar to tensor\nx = torch.tensor([1, 2, 3])\ny = x + 10  # [11, 12, 13]\n# 10 is \"broadcast\" to [10, 10, 10]\n\n# Add different shaped tensors\na = torch.tensor([[1, 2, 3],\n                  [4, 5, 6]])  # Shape: [2, 3]\nb = torch.tensor([10, 20, 30])  # Shape: [3]\n\nc = a + b  # Shape: [2, 3]\n# Result: [[11, 22, 33],\n#          [14, 25, 36]]\n# b is broadcast to [[10, 20, 30],\n#                    [10, 20, 30]]\n```\n\n**In-place operations (modifies original tensor):**\n```python\nx = torch.tensor([1.0, 2.0, 3.0])\n\n# Regular operation (creates new tensor)\ny = x + 5  # x is unchanged, y is new tensor\n\n# In-place operation (modifies x)\nx.add_(5)  # x is now [6.0, 7.0, 8.0]\n# Note the underscore _ means \"in-place\"\n```\n\n**Useful tensor operations:**\n```python\n# Reshape\nx = torch.randn(12)\ny = x.view(3, 4)  # Reshape to [3, 4]\n\n# Transpose\nmatrix = torch.randn(3, 4)\ntransposed = matrix.T  # Now [4, 3]\n\n# Concatenate\na = torch.tensor([[1, 2], [3, 4]])\nb = torch.tensor([[5, 6], [7, 8]])\nc = torch.cat([a, b], dim=0)  # Stack vertically: [4, 2]\nd = torch.cat([a, b], dim=1)  # Stack horizontally: [2, 4]\n\n# Squeeze (remove dimensions of size 1)\nx = torch.randn(1, 3, 1, 4)  # [1, 3, 1, 4]\ny = x.squeeze()               # [3, 4]\n\n# Unsqueeze (add dimension of size 1)\nx = torch.randn(3, 4)     # [3, 4]\ny = x.unsqueeze(0)        # [1, 3, 4]\nz = x.unsqueeze(1)        # [3, 1, 4]\n```\n\n**Comparison operations:**\n```python\na = torch.tensor([1, 2, 3, 4, 5])\n\n# Element-wise comparison\na > 3   # [False, False, False, True, True]\na == 3  # [False, False, True, False, False]\n\n# Filter elements\nmask = a > 3\nfiltered = a[mask]  # [4, 5]\n```\n\n**Real-world examples:**\n\n**Normalize data:**\n```python\ndata = torch.tensor([100, 200, 150, 180, 120])\nmean = data.mean()\nstd = data.std()\nnormalized = (data - mean) / std\n# Standardized data with mean=0, std=1\n```\n\n**Compute accuracy:**\n```python\npredictions = torch.tensor([1, 0, 1, 1, 0])\ntargets = torch.tensor([1, 0, 0, 1, 0])\n\ncorrect = (predictions == targets).sum().item()\naccuracy = correct / len(targets)  # 0.8 (80%)\n```\n\n**Batch normalization:**\n```python\nbatch = torch.randn(32, 10)  # 32 samples, 10 features\n\n# Normalize each feature across batch\nmean = batch.mean(dim=0)  # Mean per feature\nstd = batch.std(dim=0)    # Std per feature\nnormalized_batch = (batch - mean) / std\n```\n\n**Like:**\n- Basic operations = Calculator functions\n- Matrix multiplication = Spreadsheet formulas\n- Broadcasting = Auto-fill in Excel\n- Aggregations = SUM(), AVERAGE() in Excel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8rYObkabicO"
      },
      "source": [
        "\n",
        "\n",
        "> See! just like Numpy Arrays, but more powerful!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBOtVHO3WY_-"
      },
      "source": [
        "---\n",
        "## \ud83d\udcca **Data Representation in Deep Learning**\n",
        "\n",
        "The way data is represented depends on whether it is **structured (tabular)** or **unstructured (images)**.\n",
        "\n",
        "### \ud83d\udd391\ufe0f\u20e3 **Tabular Data (Structured Data)**\n",
        "Tabular data consists of rows and columns.\n",
        "\n",
        "Each row represents a sample and each column represents a feature.\n",
        "\n",
        "- Represented as a **2D tensor**\n",
        "- Shape: `(batch_size, number_of_features)`\n",
        "- Commonly used for tasks like regression and classification\n",
        "\n",
        "Example:\n",
        "- Features: age, salary, debt  \n",
        "- Tensor shape: `(N, 3)`\n",
        "\n",
        "### \ud83d\udd392\ufe0f\u20e3 **Image Data (Unstructured Data)**\n",
        "Image data is unstructured and contains spatial information.\n",
        "\n",
        "- Represented as a **4D tensor**\n",
        "- Shape: `(batch_size, channels, height, width)`\n",
        "- Channels represent color information:\n",
        "  - Grayscale \u2192 1 channel\n",
        "  - RGB \u2192 3 channels\n",
        "\n",
        "Example:\n",
        "- RGB image of size 224\u00d7224  \n",
        "- Tensor shape: `(N, 3, 224, 224)`\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27NITm3aWla9"
      },
      "source": [
        "## \ud83d\udccc **How to Change Dimensions in PyTorch?**\n",
        "\n",
        "Manipulating tensor shapes is essential in deep learning. PyTorch provides several functions to modify tensor dimensions.\n",
        "\n",
        "### **\ud83d\udd39 1\ufe0f\u20e3 Flatten**\n",
        "- Converts **any shape** to `(batch_size, features)`.\n",
        "- **Example:**  \n",
        "  `(batch_size, channels, height, width) \u2192 (batch_size, features)`\n",
        "\n",
        "### **\ud83d\udd39 2\ufe0f\u20e3 Squeeze**\n",
        "- **Removes dimensions** with size `1`.\n",
        "- **Example:**  \n",
        "  `(1, 32, 3, 28, 28) \u2192 (32, 3, 28, 28)`\n",
        "\n",
        "### **\ud83d\udd39 3\ufe0f\u20e3 Unsqueeze**\n",
        "- **Adds a dimension** with size `1` at a specified position.\n",
        "- **Example:**  \n",
        "  `(3, 28, 28) \u2192 (1, 3, 28, 28)`\n",
        "\n",
        "### **\ud83d\udd39 4\ufe0f\u20e3 View (works similar to reshape)**\n",
        "- **Reshapes a tensor freely** while maintaining the same number of elements.\n",
        "- **Example:**  \n",
        "  `(32, 3, 28, 28) \u2192 view(-1, 3*28*28) \u2192 (32, 3*28*28)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY0z_-JwRYWk",
        "outputId": "ec362895-9176-491b-d51e-d9e63d9b1e02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flatten: torch.Size([32, 2352])\n",
            "Squeeze: torch.Size([3, 28, 28])\n",
            "Unsqueeze: torch.Size([1, 3, 28, 28])\n",
            "View: torch.Size([32, 2352])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1\ufe0f\u20e3 Flatten - Convert any shape to (batch_size, features)\n",
        "x = torch.randn(32, 3, 28, 28)\n",
        "x_flat = x.flatten(start_dim=1)\n",
        "print(\"Flatten:\", x_flat.shape)  # (32, 2352)\n",
        "\n",
        "# 2\ufe0f\u20e3 Squeeze - Remove dimensions with size 1\n",
        "x = torch.randn(1, 3, 28, 28)\n",
        "x_sq = x.squeeze()\n",
        "print(\"Squeeze:\", x_sq.shape)  # (3, 28, 28)\n",
        "\n",
        "# 3\ufe0f\u20e3 Unsqueeze - Add a new dimension of size 1\n",
        "x = torch.randn(3, 28, 28)\n",
        "x_unsq = x.unsqueeze(0)\n",
        "print(\"Unsqueeze:\", x_unsq.shape)  # (1, 3, 28, 28)\n",
        "\n",
        "# 4\ufe0f\u20e3 View - Reshape freely while keeping same number of elements\n",
        "x = torch.randn(32, 28, 28, 3)\n",
        "x_view = x.view(32, -1)  # Flatten all except batch\n",
        "print(\"View:\", x_view.shape)  # (32, 28*28*3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n**\ud83d\udca1 Explanation:**\n\nReshaping tensors is essential for preparing data for neural networks!\n\n**Common reshaping operations:**\n\n**1. view() - Reshape tensor**\n```python\nx = torch.tensor([1, 2, 3, 4, 5, 6])  # Shape: [6]\n\n# Reshape to 2D\ny = x.view(2, 3)  # Shape: [2, 3]\n# [[1, 2, 3],\n#  [4, 5, 6]]\n\n# Reshape to 3D\nz = x.view(2, 1, 3)  # Shape: [2, 1, 3]\n```\n\n**Important:** Total elements must match!\n```python\nx = torch.randn(12)\ny = x.view(3, 4)   # \u2713 Works! 12 = 3 \u00d7 4\nz = x.view(2, 5)   # \u2717 Error! 12 \u2260 2 \u00d7 5\n```\n\n**Use -1 for automatic dimension:**\n```python\nx = torch.randn(24)\ny = x.view(4, -1)   # [4, 6] - automatically calculates 6\nz = x.view(-1, 8)   # [3, 8] - automatically calculates 3\nw = x.view(2, 3, -1) # [2, 3, 4] - automatically calculates 4\n```\n\n**2. reshape() - More flexible alternative**\n```python\nx = torch.randn(12)\ny = x.reshape(3, 4)  # Same as view() but handles edge cases better\n```\n\n**Difference between view() and reshape():**\n```python\n# view() requires contiguous memory\n# reshape() works even if memory is not contiguous\n\nx = torch.randn(3, 4)\ny = x.T  # Transposed - not contiguous\n\nz = y.view(3, 4)     # \u2717 May fail\nw = y.reshape(3, 4)  # \u2713 Always works\n```\n\n**3. squeeze() - Remove dimensions of size 1**\n```python\nx = torch.randn(1, 3, 1, 4)  # [1, 3, 1, 4]\n\ny = x.squeeze()      # [3, 4] - removes all 1s\nz = x.squeeze(0)     # [3, 1, 4] - removes dim 0 only\nw = x.squeeze(2)     # [1, 3, 4] - removes dim 2 only\n```\n\n**4. unsqueeze() - Add dimension of size 1**\n```python\nx = torch.randn(3, 4)  # [3, 4]\n\ny = x.unsqueeze(0)   # [1, 3, 4] - add at beginning\nz = x.unsqueeze(1)   # [3, 1, 4] - add in middle\nw = x.unsqueeze(2)   # [3, 4, 1] - add at end\n```\n\n**5. flatten() - Convert to 1D**\n```python\nx = torch.randn(2, 3, 4)  # [2, 3, 4]\ny = x.flatten()            # [24] - all elements in 1D\n\n# Flatten from specific dimension\nz = x.flatten(start_dim=1)  # [2, 12] - flatten dims 1 and 2\n```\n\n**6. permute() - Rearrange dimensions**\n```python\nx = torch.randn(2, 3, 4)  # [2, 3, 4]\n\n# Swap dimensions\ny = x.permute(2, 0, 1)  # [4, 2, 3]\n# What was dim 2 is now dim 0\n# What was dim 0 is now dim 1\n# What was dim 1 is now dim 2\n```\n\n**7. transpose() - Swap two dimensions**\n```python\nx = torch.randn(2, 3, 4)  # [2, 3, 4]\n\ny = x.transpose(0, 2)     # [4, 3, 2] - swap dims 0 and 2\nz = x.transpose(1, 2)     # [2, 4, 3] - swap dims 1 and 2\n\n# For 2D, can use .T\nmatrix = torch.randn(3, 4)  # [3, 4]\ntransposed = matrix.T        # [4, 3]\n```\n\n**Real-world reshaping scenarios:**\n\n**Scenario 1: Prepare data for linear layer**\n```python\n# Have: Batch of images [32, 3, 28, 28]\n# Need: Flat features [32, 2352]\n\nimages = torch.randn(32, 3, 28, 28)\nflat = images.flatten(start_dim=1)  # [32, 2352]\n# Now can pass to linear layer\n```\n\n**Scenario 2: Add batch dimension**\n```python\n# Have: Single image [3, 224, 224]\n# Need: Batch format [1, 3, 224, 224]\n\nimage = torch.randn(3, 224, 224)\nbatched = image.unsqueeze(0)  # [1, 3, 224, 224]\n# Now compatible with batch processing\n```\n\n**Scenario 3: Channels first to channels last**\n```python\n# PyTorch: [batch, channels, height, width]\n# Some libraries need: [batch, height, width, channels]\n\nx = torch.randn(32, 3, 224, 224)  # PyTorch format\ny = x.permute(0, 2, 3, 1)         # [32, 224, 224, 3]\n```\n\n**Scenario 4: Reshape for RNN**\n```python\n# Have: [batch, features]\n# Need: [sequence_length, batch, features]\n\nx = torch.randn(32, 10)           # [batch=32, features=10]\nx = x.unsqueeze(0)                # [1, 32, 10]\n# Now: [seq_len=1, batch=32, features=10]\n```\n\n**Common pitfalls:**\n\n**Pitfall 1: Element count mismatch**\n```python\nx = torch.randn(12)\ny = x.view(3, 5)  # \u2717 Error! 12 \u2260 15\n```\n\n**Pitfall 2: Using wrong dimension for operations**\n```python\nbatch = torch.randn(32, 10)\n\n# Wrong: Mean across all elements\nmean_wrong = batch.mean()  # Single value\n\n# Correct: Mean per feature\nmean_correct = batch.mean(dim=0)  # [10] values\n```\n\n**Visual examples:**\n\n**Reshaping [6] \u2192 [2, 3]:**\n```\nOriginal: [1, 2, 3, 4, 5, 6]\n\nAfter view(2, 3):\n[[1, 2, 3],\n [4, 5, 6]]\n```\n\n**Permute [2, 3, 4] \u2192 [4, 2, 3]:**\n```\nOriginal shape: [2, 3, 4]\nAfter permute(2, 0, 1): [4, 2, 3]\n\nWhat was depth (4) is now rows\nWhat was rows (2) is now columns  \nWhat was columns (3) is now depth\n```\n\n**Like:**\n- view() = Reorganizing bookshelf (same books, different arrangement)\n- squeeze() = Removing empty shelves\n- unsqueeze() = Adding empty shelves\n- flatten() = Putting all books in a single line\n- permute() = Rotating a 3D object\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q98MVwNxvNwL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkrCEfnNborH"
      },
      "source": [
        "## \ud83d\udccc **Changing Data Type or Moving Data/Model to CPU/GPU**  \n",
        "\n",
        "PyTorch allows you to **change the datatype** of a tensor and **move it between CPU and GPU** using `.to()`.  \n",
        "\n",
        "\n",
        "### \ud83d\udd39 **Change Datatype**\n",
        "Use `.to(dtype)` to convert a tensor's data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwAQnY7HWYlG",
        "outputId": "090a2dac-fd3b-46c8-cf81-28dbeb5a00c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "torch.float16\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a float32 tensor\n",
        "x = torch.tensor([1.2, 2.3, 3.4], dtype=torch.float32)\n",
        "print(x.dtype)  # Output: torch.float32\n",
        "\n",
        "# Convert to float16\n",
        "x_half = x.to(torch.float16)\n",
        "print(x_half.dtype)  # Output: torch.float16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n**\ud83d\udca1 Explanation:**\n\nManaging data types and devices (CPU/GPU) is crucial for performance and memory!\n\n**Common PyTorch data types:**\n\n**Integer types:**\n```python\n# 8-bit integer\nx = torch.tensor([1, 2, 3], dtype=torch.int8)\n\n# 16-bit integer\nx = torch.tensor([1, 2, 3], dtype=torch.int16)\n\n# 32-bit integer (default for integers)\nx = torch.tensor([1, 2, 3], dtype=torch.int32)  \n# Also: torch.int\n\n# 64-bit integer\nx = torch.tensor([1, 2, 3], dtype=torch.int64)\n# Also: torch.long\n```\n\n**Float types:**\n```python\n# 16-bit float (half precision)\nx = torch.tensor([1.0, 2.0], dtype=torch.float16)\n# Also: torch.half\n\n# 32-bit float (default for floats)\nx = torch.tensor([1.0, 2.0], dtype=torch.float32)\n# Also: torch.float\n\n# 64-bit float (double precision)\nx = torch.tensor([1.0, 2.0], dtype=torch.float64)\n# Also: torch.double\n```\n\n**Boolean:**\n```python\nx = torch.tensor([True, False, True], dtype=torch.bool)\n```\n\n**Why data types matter:**\n\n**Memory usage:**\n```python\nx_float64 = torch.randn(1000, dtype=torch.float64)  # 8 bytes per element\nx_float32 = torch.randn(1000, dtype=torch.float32)  # 4 bytes per element\nx_float16 = torch.randn(1000, dtype=torch.float16)  # 2 bytes per element\n\n# float64: 8000 bytes\n# float32: 4000 bytes (50% savings!)\n# float16: 2000 bytes (75% savings!)\n```\n\n**Precision trade-off:**\n```python\n# float64 (double): High precision, more memory\n# \u2713 Scientific computing\n# \u2713 When accuracy is critical\n\n# float32 (single): Good precision, standard\n# \u2713 Default for deep learning\n# \u2713 Good balance\n\n# float16 (half): Lower precision, less memory\n# \u2713 Faster training on modern GPUs\n# \u2713 Mixed precision training\n```\n\n**Converting data types:**\n```python\nx = torch.tensor([1.5, 2.7, 3.9])  # float32 by default\n\n# Convert to different types\nx_int = x.int()       # [1, 2, 3] - truncates decimals\nx_long = x.long()     # [1, 2, 3] - 64-bit integer\nx_double = x.double() # [1.5, 2.7, 3.9] - 64-bit float\nx_half = x.half()     # [1.5, 2.7, 3.9] - 16-bit float\n\n# Using .to()\nx_int = x.to(torch.int32)\nx_long = x.to(torch.int64)\n```\n\n**CPU vs GPU (Device management):**\n\n**Check GPU availability:**\n```python\nimport torch\n\nif torch.cuda.is_available():\n    print(\"GPU is available!\")\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    print(\"GPU not available, using CPU\")\n```\n\n**Move tensors to GPU:**\n```python\n# Create tensor on CPU (default)\nx = torch.randn(3, 4)\nprint(x.device)  # cpu\n\n# Move to GPU\nif torch.cuda.is_available():\n    x_gpu = x.cuda()  # Move to GPU\n    # Or:\n    x_gpu = x.to('cuda')\n    print(x_gpu.device)  # cuda:0\n```\n\n**Move back to CPU:**\n```python\nx_gpu = torch.randn(3, 4).cuda()\nx_cpu = x_gpu.cpu()  # Move back to CPU\n# Or:\nx_cpu = x_gpu.to('cpu')\n```\n\n**Automatic device selection:**\n```python\n# Best practice: Automatically choose GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Create tensor on chosen device\nx = torch.randn(3, 4, device=device)\n\n# Or move existing tensor\ny = torch.randn(3, 4)\ny = y.to(device)\n```\n\n**Performance comparison:**\n\n**CPU:**\n```python\nx_cpu = torch.randn(1000, 1000)\ny_cpu = torch.randn(1000, 1000)\n\n# Matrix multiplication on CPU\nresult = x_cpu @ y_cpu  # ~10ms\n```\n\n**GPU:**\n```python\nx_gpu = torch.randn(1000, 1000, device='cuda')\ny_gpu = torch.randn(1000, 1000, device='cuda')\n\n# Matrix multiplication on GPU\nresult = x_gpu @ y_gpu  # ~0.1ms (100x faster!)\n```\n\n**Important rules:**\n\n**Rule 1: Operations require same device**\n```python\nx_cpu = torch.randn(3, 4)\ny_gpu = torch.randn(3, 4).cuda()\n\nz = x_cpu + y_gpu  # \u2717 Error! Different devices\n```\n\n**Solution:**\n```python\n# Move both to same device\nx_gpu = x_cpu.cuda()\nz = x_gpu + y_gpu  # \u2713 Works!\n```\n\n**Rule 2: Model and data must be on same device**\n```python\nmodel = MyModel().cuda()  # Model on GPU\ndata = torch.randn(32, 10)  # Data on CPU\n\noutput = model(data)  # \u2717 Error!\n\n# Correct:\ndata = data.cuda()\noutput = model(data)  # \u2713 Works!\n```\n\n**Real-world workflow:**\n\n**Training setup:**\n```python\n# 1. Choose device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 2. Move model to device\nmodel = MyModel().to(device)\n\n# 3. Training loop\nfor batch_data, batch_labels in dataloader:\n    # Move data to device\n    batch_data = batch_data.to(device)\n    batch_labels = batch_labels.to(device)\n    \n    # Forward pass\n    outputs = model(batch_data)\n    loss = criterion(outputs, batch_labels)\n    \n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n**Memory considerations:**\n\n**GPU memory is limited!**\n```python\n# Check GPU memory\nif torch.cuda.is_available():\n    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n    print(f\"Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n```\n\n**Clear GPU memory:**\n```python\n# Delete tensors\ndel x_gpu, y_gpu\n\n# Clear cache\ntorch.cuda.empty_cache()\n```\n\n**Mixed precision training (advanced):**\n```python\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor data, target in dataloader:\n    optimizer.zero_grad()\n    \n    # Use automatic mixed precision\n    with autocast():\n        output = model(data)\n        loss = criterion(output, target)\n    \n    # Scale loss and backprop\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n```\n\n**Benefits of mixed precision:**\n- 50% less memory usage\n- 2-3x faster training\n- Minimal accuracy loss\n\n**Common data type conversions:**\n\n**For labels (classification):**\n```python\nlabels = torch.tensor([0, 1, 2, 1, 0])  # int64 by default\n# Usually fine for CrossEntropyLoss\n```\n\n**For features:**\n```python\nfeatures = torch.randn(100, 10)  # float32 by default\n# Standard for neural networks\n```\n\n**Converting NumPy to PyTorch:**\n```python\nimport numpy as np\n\n# NumPy array\nnp_array = np.array([1.0, 2.0, 3.0])\n\n# Convert to PyTorch\ntensor = torch.from_numpy(np_array)\n\n# Convert back to NumPy\nnp_array_back = tensor.numpy()\n```\n\n**Like:**\n- Data types = Choosing precision in measurements (mm vs cm vs m)\n- CPU/GPU = Using calculator vs supercomputer\n- Same device requirement = All workers must be in same office\n- Mixed precision = Using different decimal precision where needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MOJ8tx0ceVL"
      },
      "source": [
        "### \ud83d\udd39 **Move Tensors to GPU (if available)**\n",
        "**GPUs are faster and more efficient** in most cases when training or inferencing deep learning models.\n",
        "\n",
        "Use `.to(device)` to move a tensor to GPU for faster computation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Iv00vultxnB",
        "outputId": "4ada2c54-8d8d-4fe1-da07-6775b53aa5ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Automatically select CPU or GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create a tensor and move it to GPU\n",
        "x_gpu = x.to(device)\n",
        "print(x_gpu.device)  # Output: cuda:0 (if GPU is available) or cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n**\ud83d\udca1 Explanation:**\n\nAutomatically selects the best available device (GPU if available, otherwise CPU)!\n\n**The device selection pattern:**\n```python\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n```\n\n**What this does:**\n\n**If GPU available:**\n```python\ntorch.cuda.is_available()  # True\ndevice = torch.device('cuda')\nprint(device)  # cuda\n```\n\n**If GPU not available:**\n```python\ntorch.cuda.is_available()  # False\ndevice = torch.device('cpu')\nprint(device)  # cpu\n```\n\n**Why this pattern is best practice:**\n\n**\u274c Bad: Hard-code device**\n```python\ndevice = torch.device('cuda')  # Fails if no GPU!\n```\n\n**\u2713 Good: Automatic selection**\n```python\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# Works everywhere: laptops, servers, cloud\n```\n\n**Complete usage example:**\n\n```python\n# 1. Select device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# 2. Create model and move to device\nmodel = NeuralNetwork()\nmodel = model.to(device)\n\n# 3. Training loop\nfor batch_data, batch_labels in dataloader:\n    # Move data to device\n    batch_data = batch_data.to(device)\n    batch_labels = batch_labels.to(device)\n    \n    # Forward pass (all on same device)\n    outputs = model(batch_data)\n    loss = criterion(outputs, batch_labels)\n    \n    # Backward pass\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n**GPU information:**\n```python\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Count: {torch.cuda.device_count()}\")\n    print(f\"CUDA Version: {torch.version.cuda}\")\nelse:\n    print(\"No GPU available\")\n```\n\n**Multiple GPU handling:**\n```python\nif torch.cuda.is_available():\n    # Use specific GPU\n    device = torch.device('cuda:0')  # First GPU\n    # device = torch.device('cuda:1')  # Second GPU\nelse:\n    device = torch.device('cpu')\n```\n\n**Performance implications:**\n\n**CPU only (no GPU):**\n```python\ndevice = torch.device('cpu')\n# Training time: 10 hours\n# Good for: Small models, testing, debugging\n```\n\n**With GPU:**\n```python\ndevice = torch.device('cuda')\n# Training time: 30 minutes (20x faster!)\n# Good for: Production training, large models\n```\n\n**Real-world example:**\n\n```python\nimport torch\nimport torch.nn as nn\n\n# Device selection\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Training on: {device}\")\n\n# Define model\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(10, 64)\n        self.fc2 = nn.Linear(64, 2)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Move model to device\nmodel = SimpleNN().to(device)\n\n# Create sample data\ndata = torch.randn(32, 10).to(device)  # Move to device\nlabels = torch.randint(0, 2, (32,)).to(device)  # Move to device\n\n# Forward pass (all on same device)\noutputs = model(data)\nprint(f\"Outputs device: {outputs.device}\")  # Same as model and data\n```\n\n**Common errors and solutions:**\n\n**Error 1: Tensor on wrong device**\n```python\nmodel = model.to('cuda')\ndata = torch.randn(32, 10)  # Still on CPU\n\noutput = model(data)  # \u2717 Error: Expected CUDA tensor\n\n# Solution:\ndata = data.to(device)\noutput = model(data)  # \u2713 Works\n```\n\n**Error 2: Mixing devices**\n```python\nx = torch.randn(10).to('cuda')\ny = torch.randn(10)  # CPU\n\nz = x + y  # \u2717 Error: Different devices\n\n# Solution:\ny = y.to('cuda')\nz = x + y  # \u2713 Works\n```\n\n**Best practices:**\n\n**1. Always use device variable:**\n```python\n# \u2713 Good\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntensor = tensor.to(device)\n\n# \u2717 Bad\ntensor = tensor.to('cuda')  # Hardcoded\n```\n\n**2. Move everything to device:**\n```python\n# Model\nmodel = model.to(device)\n\n# Data\ndata = data.to(device)\nlabels = labels.to(device)\n\n# Loss values for monitoring\nloss_value = loss.item()  # Move to CPU for printing\n```\n\n**3. Be consistent:**\n```python\n# All training code uses same device\nfor batch in dataloader:\n    batch = batch.to(device)  # Consistent device usage\n    output = model(batch)\n    # ... rest of training\n```\n\n**Like:**\n- Device selection = Choosing workspace (home office vs corporate office)\n- .to(device) = Moving all tools to your workspace\n- Same device requirement = All team members in same building\n- GPU = High-speed assembly line\n- CPU = Manual workshop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqEVOLXed3ut"
      },
      "source": [
        "Note: When training a model, always move BOTH the model and data to the same device. Otherwise, you will get an error like this:\n",
        "\n",
        "`RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8DN_nVlUbod"
      },
      "source": [
        "## **\ud83d\udccc PyTorch Workflow Organization**\n",
        "\n",
        "### **It consists of 4 main components:**\n",
        "1\ufe0f\u20e3 **Dataset Class**  \n",
        "- Handles loading and preprocessing data.  \n",
        "- Converts raw data (e.g., images, CSVs) into model-ready tensors.  \n",
        "\n",
        "2\ufe0f\u20e3 **Model Class**  \n",
        "- Defines the architecture of your neural network (e.g., layers, activations).  \n",
        "\n",
        "3\ufe0f\u20e3 **Training Loop**  \n",
        "- Updates model weights using backpropagation and optimizers.  \n",
        "- Computes the loss for every batch and adjusts the parameters to minimize it.  \n",
        "\n",
        "4\ufe0f\u20e3 **Validation Loop**  \n",
        "- Evaluates the model's performance on a validation set.  \n",
        "- Does not update weights but computes metrics like accuracy or loss.  \n",
        "\n",
        "\n",
        "\n",
        "### **\ud83d\udccc Note:**\n",
        "All the labs will follow this structure. You will just modify the content for different tasks, such as changing datasets, architectures, or loss functions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXG9aSXCYaza"
      },
      "source": [
        "## 1\ufe0f\u20e3 **Dataset Class**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOwm0-zsulwK"
      },
      "source": [
        "In PyTorch, a **Dataset Class** is responsible for transforming raw data into samples that are ready to be used by a model.  \n",
        "Each sample returned consists of:\n",
        "- An **input** (features or image)\n",
        "- Its corresponding **label**  \n",
        "\n",
        "\n",
        "\n",
        "## \ud83d\udd39 For Tabular Data (Using `TensorDataset`)\n",
        "\n",
        "When working with **tabular data** (e.g., CSV files already converted to tensors), we can use\n",
        "`TensorDataset` to pair input features with their labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNlgbwZPnD6H",
        "outputId": "9b31d49e-1783-4d42-beff-43809ed7426d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of one sample: torch.Size([30])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# load data\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# transform to tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test  = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# TensorDataset pairs input features (X) with their corresponding labels (y)\n",
        "# Each item in the dataset is returned as (X[i], y[i])\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Access a single sample from the dataset\n",
        "# This helps verify the shape of one data sample\n",
        "first_sample, _ = train_dataset[0]\n",
        "print(f\"Shape of one sample: {first_sample.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n**\ud83d\udca1 Explanation:**\n\nCreates a **custom PyTorch Dataset** for tabular data!\n\n**What is a Dataset Class?**\nA Dataset is a class that:\n1. Loads your data\n2. Applies transformations\n3. Returns samples when requested\n\n**Required methods:**\n\n**1. `__init__`: Initialize and load data**\n```python\ndef __init__(self):\n    # Load data\n    # Preprocess\n    # Store in memory\n```\n\n**2. `__len__`: Return total number of samples**\n```python\ndef __len__(self):\n    return len(self.data)\n```\n\n**3. `__getitem__`: Return one sample**\n```python\ndef __getitem__(self, idx):\n    return self.data[idx], self.labels[idx]\n```\n\n**Complete example:**\n\n```python\nfrom torch.utils.data import Dataset\nimport torch\n\nclass BreastCancerDataset(Dataset):\n    def __init__(self):\n        # Load data\n        from sklearn.datasets import load_breast_cancer\n        data = load_breast_cancer()\n        \n        # Store features and labels as tensors\n        self.X = torch.FloatTensor(data.data)\n        self.y = torch.LongTensor(data.target)\n    \n    def __len__(self):\n        # Return total number of samples\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        # Return sample at index idx\n        return self.X[idx], self.y[idx]\n\n# Usage\ndataset = BreastCancerDataset()\nprint(f\"Dataset size: {len(dataset)}\")  # 569 samples\n\n# Get first sample\nfeatures, label = dataset[0]\nprint(f\"Features shape: {features.shape}\")  # [30]\nprint(f\"Label: {label}\")  # 0 or 1\n```\n\n**Why use Dataset classes?**\n\n**Without Dataset (messy):**\n```python\n# Scattered data handling\nX = load_data()\ny = load_labels()\nX = preprocess(X)\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.LongTensor(y)\n# ... complicated indexing and batching\n```\n\n**With Dataset (clean):**\n```python\ndataset = MyDataset()  # All logic in one place\ndataloader = DataLoader(dataset, batch_size=32)\nfor batch_X, batch_y in dataloader:\n    # Clean, simple training loop\n```\n\n**Custom Dataset with transformations:**\n\n```python\nclass CustomDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data = pd.read_csv(data_path)\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        # Get sample\n        sample = self.data.iloc[idx]\n        \n        # Apply transformations\n        if self.transform:\n            sample = self.transform(sample)\n        \n        features = torch.FloatTensor(sample[:-1])\n        label = torch.LongTensor([sample[-1]])\n        \n        return features, label\n```\n\n**Dataset with train/test split:**\n\n```python\nclass SplitDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.FloatTensor(X)\n        self.y = torch.LongTensor(y)\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# Split data first\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Create separate datasets\ntrain_dataset = SplitDataset(X_train, y_train)\ntest_dataset = SplitDataset(X_test, y_test)\n```\n\n**Dataset with normalization:**\n\n```python\nclass NormalizedDataset(Dataset):\n    def __init__(self, X, y, normalize=True):\n        self.X = torch.FloatTensor(X)\n        self.y = torch.LongTensor(y)\n        \n        if normalize:\n            # Compute mean and std\n            self.mean = self.X.mean(dim=0)\n            self.std = self.X.std(dim=0)\n            \n            # Normalize\n            self.X = (self.X - self.mean) / self.std\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n```\n\n**Real-world dataset patterns:**\n\n**Pattern 1: CSV dataset**\n```python\nclass CSVDataset(Dataset):\n    def __init__(self, csv_file):\n        self.data = pd.read_csv(csv_file)\n        \n        # Separate features and labels\n        self.X = self.data.iloc[:, :-1].values\n        self.y = self.data.iloc[:, -1].values\n        \n        # Convert to tensors\n        self.X = torch.FloatTensor(self.X)\n        self.y = torch.LongTensor(self.y)\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n```\n\n**Pattern 2: Memory-efficient large dataset**\n```python\nclass LazyDataset(Dataset):\n    def __init__(self, file_list):\n        self.file_list = file_list\n    \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, idx):\n        # Load data only when needed (lazy loading)\n        data = load_file(self.file_list[idx])\n        return process(data)\n```\n\n**Pattern 3: Augmented dataset**\n```python\nclass AugmentedDataset(Dataset):\n    def __init__(self, X, y, augment_prob=0.5):\n        self.X = X\n        self.y = y\n        self.augment_prob = augment_prob\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        x, y = self.X[idx], self.y[idx]\n        \n        # Random augmentation\n        if random.random() < self.augment_prob:\n            x = self.augment(x)\n        \n        return x, y\n```\n\n**Testing your dataset:**\n\n```python\n# Create dataset\ndataset = MyDataset()\n\n# Test __len__\nprint(f\"Dataset size: {len(dataset)}\")\n\n# Test __getitem__\nsample, label = dataset[0]\nprint(f\"Sample shape: {sample.shape}\")\nprint(f\"Label: {label}\")\n\n# Test random access\nindices = [0, 10, 100]\nfor idx in indices:\n    sample, label = dataset[idx]\n    print(f\"Sample {idx}: {sample.shape}, label: {label}\")\n\n# Test iteration\nfor i, (x, y) in enumerate(dataset):\n    if i >= 5:  # Test first 5 samples\n        break\n    print(f\"Batch {i}: x={x.shape}, y={y}\")\n```\n\n**Common errors:**\n\n**Error 1: Wrong tensor type**\n```python\n# \u2717 Bad\nself.y = torch.FloatTensor(y)  # Labels should be Long\n\n# \u2713 Good\nself.y = torch.LongTensor(y)  # For classification\n```\n\n**Error 2: Not returning tuple**\n```python\n# \u2717 Bad\ndef __getitem__(self, idx):\n    return self.X[idx]  # Only features\n\n# \u2713 Good\ndef __getitem__(self, idx):\n    return self.X[idx], self.y[idx]  # Features and label\n```\n\n**Error 3: Dimension mismatch**\n```python\n# Data: [569, 30]\n# But returning wrong shape\n\n# \u2717 Bad\nreturn self.X[idx].squeeze()  # Might remove needed dimension\n\n# \u2713 Good  \nreturn self.X[idx]  # Keep original shape\n```\n\n**Like:**\n- Dataset = Library catalog system\n- `__len__` = Total number of books\n- `__getitem__` = Retrieving a specific book by ID\n- DataLoader = Automated book delivery service (batches)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMPiHVpcxYt5"
      },
      "source": [
        "## \ud83d\udd39 For Image Data (Using Built-in Datasets)\n",
        "\n",
        "In PyTorch, image datasets can be:\n",
        "- **Built-in datasets** provided by PyTorch (e.g., MNIST, CIFAR-10)\n",
        "- **Custom datasets** created for data that is not provided in a ready-made format\n",
        "\n",
        "In **Stage 2**, we'll use **built-in datasets**\n",
        "\n",
        "In **Stage 3**, we'll create **custom Dataset classes** for our own image data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCinzfXNyUON",
        "outputId": "ae77faca-22e6-452a-b3bb-3482c9160e4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.91M/9.91M [00:00<00:00, 18.0MB/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.9k/28.9k [00:00<00:00, 481kB/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.65M/1.65M [00:00<00:00, 4.49MB/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.54k/4.54k [00:00<00:00, 7.54MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Image shape: torch.Size([1, 28, 28])\n",
            "Label: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "# Training dataset\n",
        "train_dataset = MNIST(\n",
        "    root='./datasets',     # Dataset storage path\n",
        "    train=True,            # Use training data\n",
        "    transform=to_tensor,   # Convert images to tensors\n",
        "    download=True          # Download if not available\n",
        ")\n",
        "\n",
        "# Testing dataset\n",
        "test_dataset = MNIST(\n",
        "    root='./datasets',     # Dataset storage path\n",
        "    train=False,           # Use test data\n",
        "    transform=to_tensor,   # Convert images to tensors\n",
        "    download=True          # Download if not available\n",
        ")\n",
        "\n",
        "# print one sample from the dataset\n",
        "# Each sample consists of an image tensor and its label\n",
        "sample_image, sample_label = train_dataset[0]\n",
        "\n",
        "print(f\"\\n Image shape: {sample_image.shape}\")  # (1, 28, 28)\n",
        "print(f\"Label: {sample_label}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n**\ud83d\udca1 Explanation:**\n\nUsing **built-in PyTorch image datasets** (MNIST example)!\n\n**torchvision.datasets:**\nPyTorch provides many pre-built datasets for computer vision!\n\n**Available datasets:**\n- **MNIST**: Handwritten digits (28x28 grayscale)\n- **Fashion-MNIST**: Clothing items (28x28 grayscale)\n- **CIFAR-10**: Objects in 10 classes (32x32 color)\n- **CIFAR-100**: Objects in 100 classes (32x32 color)\n- **ImageNet**: 1000 object classes (various sizes)\n\n**MNIST example:**\n```python\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # Convert to tensor\n    transforms.Normalize((0.5,), (0.5,))  # Normalize\n])\n\n# Download and load training data\ntrain_dataset = MNIST(\n    root='./data',           # Where to save/load\n    train=True,              # Training set\n    download=True,           # Download if not present\n    transform=transform      # Apply transformations\n)\n\n# Download and load test data\ntest_dataset = MNIST(\n    root='./data',\n    train=False,             # Test set\n    download=True,\n    transform=transform\n)\n\nprint(f\"Training samples: {len(train_dataset)}\")  # 60,000\nprint(f\"Test samples: {len(test_dataset)}\")       # 10,000\n```\n\n**Understanding transformations:**\n\n**transforms.ToTensor():**\n```python\n# Converts PIL Image or numpy array to tensor\n# Also scales values from [0, 255] to [0, 1]\n\n# Before: PIL Image [28, 28] with values 0-255\n# After: Tensor [1, 28, 28] with values 0.0-1.0\n```\n\n**transforms.Normalize():**\n```python\n# Normalize with mean and std\ntransforms.Normalize((mean,), (std,))\n\n# For grayscale (1 channel):\ntransforms.Normalize((0.5,), (0.5,))\n\n# For RGB (3 channels):\ntransforms.Normalize(\n    (0.485, 0.456, 0.406),  # ImageNet means\n    (0.229, 0.224, 0.225)   # ImageNet stds\n)\n\n# Formula: normalized = (image - mean) / std\n```\n\n**Common transform pipelines:**\n\n**Simple transforms:**\n```python\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n```\n\n**Data augmentation (training):**\n```python\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.RandomCrop(28, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n```\n\n**Test transforms (no augmentation):**\n```python\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n```\n\n**Using the dataset:**\n\n```python\n# Get one sample\nimage, label = train_dataset[0]\nprint(f\"Image shape: {image.shape}\")  # [1, 28, 28]\nprint(f\"Label: {label}\")               # 0-9\n\n# Visualize\nimport matplotlib.pyplot as plt\n\n# Convert to numpy and squeeze\nimg = image.squeeze().numpy()\nplt.imshow(img, cmap='gray')\nplt.title(f\"Label: {label}\")\nplt.show()\n```\n\n**Complete image classification setup:**\n\n```python\nfrom torchvision.datasets import CIFAR10\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n# Transforms\ntransform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Datasets\ntrain_dataset = CIFAR10(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transform_train\n)\n\ntest_dataset = CIFAR10(\n    root='./data',\n    train=False,\n    download=True,\n    transform=transform_test\n)\n\n# DataLoaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=128,\n    shuffle=True,\n    num_workers=2\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=128,\n    shuffle=False,\n    num_workers=2\n)\n\n# Training loop\nfor images, labels in train_loader:\n    # images: [128, 3, 32, 32]\n    # labels: [128]\n    outputs = model(images)\n    loss = criterion(outputs, labels)\n    # ... backprop and optimization\n```\n\n**Custom image dataset:**\n\n```python\nfrom torchvision.datasets import ImageFolder\n\n# Directory structure:\n# data/\n#   \u251c\u2500\u2500 train/\n#   \u2502   \u251c\u2500\u2500 class1/\n#   \u2502   \u2502   \u251c\u2500\u2500 img1.jpg\n#   \u2502   \u2502   \u2514\u2500\u2500 img2.jpg\n#   \u2502   \u2514\u2500\u2500 class2/\n#   \u2502       \u251c\u2500\u2500 img1.jpg\n#   \u2502       \u2514\u2500\u2500 img2.jpg\n#   \u2514\u2500\u2500 test/\n#       \u2514\u2500\u2500 ...\n\ntrain_dataset = ImageFolder(\n    root='data/train',\n    transform=transform\n)\n\n# Automatically assigns labels based on folder names\n# class1 \u2192 0, class2 \u2192 1, etc.\n```\n\n**Like:**\n- Built-in datasets = Pre-packaged meal kits (everything included)\n- Custom datasets = Cooking from scratch (more control)\n- Transforms = Recipe instructions (how to prepare)\n- DataLoader = Serving dishes (presenting in batches)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O5WZp8HJ6fK"
      },
      "source": [
        "\n",
        "Right now, our **Dataset Class** loads **one sample at a time** when we call:\n",
        "```python\n",
        "sample_image, sample_label = train_dataset[0]  # Loads only one sample\n",
        "```\n",
        "\u2705 **That\u2019s great for understanding**, but when training a model, we need to process **multiple samples at once** for efficiency.\n",
        "\n",
        "\u274c **Problem**: We need batches, not single samples.  \n",
        "\u2705 **Solution**: We use `DataLoader` to handle batching automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CEW3kknM6t3"
      },
      "source": [
        "## \ud83d\udccc **What are DataLoaders ?**\n",
        "\n",
        "<img src=\"https://i.imgur.com/aHE3lnE.png\" width=\"70%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8GKnrdjFPtH"
      },
      "source": [
        "\n",
        "A **DataLoader** is a PyTorch utility that takes a Dataset and does:\n",
        "- **Batching**: Groups multiple samples together for faster processing.\n",
        "- **Shuffling**: Randomizes data order to improve training.\n",
        "- **Multi-threading**: Loads data efficiently in parallel.\n",
        "\n",
        "| **Argument**     | **Description** |\n",
        "|-----------------|---------------|\n",
        "| `dataset` | The dataset object (e.g., `train_dataset`) |\n",
        "| `batch_size` | Number of samples per batch (e.g., `32`) |\n",
        "| `shuffle` | Whether to **randomly shuffle** data each epoch (`True` = better for training) |\n",
        "| `num_workers` | Number of parallel **CPU workers** to load data faster |\n",
        "| `collate_fn` | A function to **customize how data is stacked** (useful when data has variable sizes) |\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qivcbu_CNXfh",
        "outputId": "fa394753-258e-4f79-8adf-6575b0d6b238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training batch input shape: torch.Size([32, 1, 28, 28])\n",
            "Training batch labels shape: torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# DataLoader for training data\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "# DataLoader for test/validation data\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "# Get the first batch from the training DataLoader\n",
        "X_batch, y_batch = next(iter(train_loader))\n",
        "print(f\"Training batch input shape: {X_batch.shape}\")\n",
        "print(f\"Training batch labels shape: {y_batch.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n**\ud83d\udca1 Explanation:**\n\n**DataLoader** handles batching, shuffling, and parallel loading of your data!\n\n**What is DataLoader?**\nTakes a Dataset and creates batches of data efficiently.\n\n**Basic usage:**\n```python\nfrom torch.utils.data import DataLoader\n\n# Create dataset first\ndataset = MyDataset()\n\n# Create DataLoader\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=0\n)\n\n# Iterate through batches\nfor batch_data, batch_labels in dataloader:\n    print(batch_data.shape)   # [32, features]\n    print(batch_labels.shape) # [32]\n```\n\n**Key parameters explained:**\n\n**1. batch_size:**\n```python\n# Dataset: 1000 samples\n# batch_size=32\n\n# Creates: 1000 // 32 = 31 full batches of 32\n#          + 1 partial batch of 8\n# Total: 32 batches\n\ndataloader = DataLoader(dataset, batch_size=32)\n```\n\n**2. shuffle:**\n```python\n# shuffle=True: Randomizes order each epoch\ndataloader = DataLoader(dataset, shuffle=True)\n\n# shuffle=False: Same order every epoch\ndataloader = DataLoader(dataset, shuffle=False)\n```\n\n**Why shuffle matters:**\n\n**Without shuffling:**\n```python\n# Epoch 1: [sample0, sample1, sample2, ...]\n# Epoch 2: [sample0, sample1, sample2, ...]\n# Model might learn data order! \u2717\n```\n\n**With shuffling:**\n```python\n# Epoch 1: [sample42, sample7, sample103, ...]\n# Epoch 2: [sample91, sample3, sample67, ...]\n# Model learns patterns, not order! \u2713\n```\n\n**3. num_workers:**\n```python\n# num_workers=0: Single process (slow)\ndataloader = DataLoader(dataset, num_workers=0)\n\n# num_workers=4: 4 parallel workers (fast)\ndataloader = DataLoader(dataset, num_workers=4)\n```\n\n**Performance comparison:**\n```\nnum_workers=0:  1x speed (baseline)\nnum_workers=2:  1.8x speed\nnum_workers=4:  3.5x speed\nnum_workers=8:  6x speed (diminishing returns)\n```\n\n**4. drop_last:**\n```python\n# Dataset: 100 samples, batch_size=32\n\n# drop_last=False (default)\n# Creates batches: [32, 32, 32, 4]\n# Last batch has only 4 samples\n\n# drop_last=True\n# Creates batches: [32, 32, 32]\n# Drops last incomplete batch\n```\n\n**5. pin_memory:**\n```python\n# For GPU training\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    pin_memory=True  # Faster GPU transfer\n)\n```\n\n**Complete DataLoader example:**\n\n```python\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\n\nclass MyDataset(Dataset):\n    def __init__(self, size=1000):\n        self.data = torch.randn(size, 10)\n        self.labels = torch.randint(0, 2, (size,))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# Create dataset\ndataset = MyDataset(size=1000)\n\n# Create DataLoader\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=2,\n    drop_last=False,\n    pin_memory=True\n)\n\n# Training loop\nfor epoch in range(10):\n    for batch_idx, (data, labels) in enumerate(dataloader):\n        # data shape: [32, 10]\n        # labels shape: [32]\n        \n        outputs = model(data)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n```\n\n**Train vs Test DataLoaders:**\n\n```python\n# Training: Shuffle for better learning\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    shuffle=True,      # \u2190 Shuffle!\n    num_workers=4\n)\n\n# Testing: Don't shuffle (consistent results)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=64,     # Can use larger batch (no gradients)\n    shuffle=False,     # \u2190 No shuffle!\n    num_workers=4\n)\n```\n\n**DataLoader output shapes:**\n\n```python\n# Dataset: 1000 samples, 10 features, 3 classes\ndataset = MyDataset()  # Each item: (10,), label\n\ndataloader = DataLoader(dataset, batch_size=32)\n\nfor batch_data, batch_labels in dataloader:\n    print(batch_data.shape)   # torch.Size([32, 10])\n    print(batch_labels.shape) # torch.Size([32])\n    break\n```\n\n**Automatic batching:**\n\n```\nDataset samples (individual):\nsample 0: [10 features], label 0\nsample 1: [10 features], label 1\nsample 2: [10 features], label 2\n...\nsample 31: [10 features], label 1\n\nDataLoader automatically stacks into batch:\nbatch_data:   [32, 10] \u2190 32 samples stacked\nbatch_labels: [32]     \u2190 32 labels stacked\n```\n\n**Custom collate function (advanced):**\n\n```python\ndef custom_collate(batch):\n    # batch is list of (data, label) tuples\n    data = [item[0] for item in batch]\n    labels = [item[1] for item in batch]\n    \n    # Custom batching logic\n    data = torch.stack(data)\n    labels = torch.tensor(labels)\n    \n    return data, labels\n\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    collate_fn=custom_collate\n)\n```\n\n**Memory considerations:**\n\n```python\n# Large batch: Fast but uses more memory\ndataloader = DataLoader(dataset, batch_size=256)\n\n# Small batch: Slower but uses less memory\ndataloader = DataLoader(dataset, batch_size=16)\n\n# Finding optimal batch size:\n# Try: 16, 32, 64, 128, 256\n# Use largest that fits in GPU memory\n```\n\n**Parallel loading benefits:**\n\n**Sequential loading (num_workers=0):**\n```\nLoad batch 1 \u2192 Process batch 1 \u2192 Load batch 2 \u2192 Process batch 2\n   \u2191 GPU idle      \u2191 CPU idle       \u2191 GPU idle     \u2191 CPU idle\n```\n\n**Parallel loading (num_workers=4):**\n```\nWorkers loading batches 2,3,4,5 while GPU processes batch 1\n   \u2191 No idle time!\n```\n\n**Common patterns:**\n\n**Pattern 1: Standard training**\n```python\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True\n)\n\nfor epoch in range(num_epochs):\n    for data, labels in train_loader:\n        # Train on batch\n        ...\n```\n\n**Pattern 2: Validation**\n```python\n@torch.no_grad()  # No gradients needed\ndef validate(model, val_loader):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    \n    for data, labels in val_loader:\n        outputs = model(data)\n        loss = criterion(outputs, labels)\n        total_loss += loss.item()\n        \n        predictions = outputs.argmax(dim=1)\n        correct += (predictions == labels).sum().item()\n    \n    return total_loss / len(val_loader), correct / len(val_loader.dataset)\n```\n\n**Pattern 3: Inference**\n```python\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=128,  # Larger batch (no gradients)\n    shuffle=False,\n    num_workers=4\n)\n\npredictions = []\nwith torch.no_grad():\n    for data, _ in test_loader:\n        output = model(data)\n        pred = output.argmax(dim=1)\n        predictions.extend(pred.cpu().numpy())\n```\n\n**Like:**\n- Dataset = Library (books)\n- DataLoader = Book delivery service\n  - batch_size = Books per delivery\n  - shuffle = Random vs ordered delivery\n  - num_workers = Number of delivery people\n  - drop_last = What to do with incomplete last delivery\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApKlCp-MvZVt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU57FGt_Ykoj"
      },
      "source": [
        "## 2\ufe0f\u20e3 **Model Class**\n",
        "\n",
        "In PyTorch, `nn.Linear(in_features, out_features)`\n",
        "\n",
        "creates a **fully connected (dense) layer** that applies a linear transformation:\n",
        "\n",
        "$y = xW^T + b$\n",
        "\n",
        "\n",
        "- **`in_features`**: number of input features  \n",
        "- **`out_features`**: number of neurons (outputs) in the layer  \n",
        "- The layer automatically creates learnable **weights** and **bias**.\n",
        "\n",
        "\u2705 Example: a layer that takes 3 input features and outputs 1 value:\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c42PESt1sUw0",
        "outputId": "b85c1621-2c9f-41fa-abbc-70196de8022b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([4, 3])\n",
            "Output shape: torch.Size([4, 1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create a linear layer: 3 input features -> 1 output\n",
        "linear = nn.Linear(in_features=3, out_features=1)\n",
        "\n",
        "# Example input: batch of 4 samples, each with 3 features\n",
        "x = torch.randn(4, 3)\n",
        "\n",
        "# Forward through the layer\n",
        "y = linear(x)\n",
        "\n",
        "print(\"Input shape:\", x.shape)   # torch.Size([4, 3])\n",
        "print(\"Output shape:\", y.shape)  # torch.Size([4, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNCHp3ihvl6s"
      },
      "source": [
        "### **\ud83d\udccc Key Components of Model Class:**\n",
        "####1\ufe0f\u20e3 **Define Layers (`__init__` method):**  \n",
        "\n",
        "Inside the `__init__` method, we define **what the neural network looks like**. This includes:\n",
        "\n",
        "- **Number of layers**  \n",
        "  How many linear (`nn.Linear`) layers the model has (depth of the network).\n",
        "\n",
        "- **Hidden layer sizes**  \n",
        "  How many neurons each hidden layer contains.\n",
        "\n",
        "- **Activation functions**  \n",
        "  Activation functions introduce **non-linearity**, allowing the network to learn complex patterns.\n",
        "  - Common choice for hidden layers: **ReLU**\n",
        "\n",
        "- **Output activation function**  \n",
        "  The activation used at the final layer depends on the task:\n",
        "  - **Binary classification** \u2192 `Sigmoid`\n",
        "  - **Multiclass classification** \u2192 `Softmax`\n",
        "  - **Regression** \u2192 No activation (linear output)\n",
        "\n",
        "\ud83d\udccc **Important:**  \n",
        "Hidden layers usually use ReLU, while the **output layer activation is task-dependent**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWfyCtXWsTgF"
      },
      "source": [
        "#### 2\ufe0f\u20e3 **Forward Pass (`forward` method):**\n",
        "\n",
        "The `forward()` method defines **how the input data flows through the model** to produce the final output.\n",
        "\n",
        "- The input tensor is passed through each layer **in order**.\n",
        "- Activation functions are applied after linear layers to introduce **non-linearity**.\n",
        "- The final layer produces the model\u2019s prediction.\n",
        "\n",
        "\ud83d\udccc **Note:**  \n",
        "During training, PyTorch automatically tracks all operations in the `forward()` method to compute gradients during backpropagation.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk6qfrKJK88F"
      },
      "source": [
        "\u2705 **Example: One neural network layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmUbfIKiZc8X"
      },
      "source": [
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IAkZWzDOYGaiu3e47rEMgQ.png\" width=\"60%\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Opi3_J4dE_pT"
      },
      "outputs": [],
      "source": [
        "class NN1Layer(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim):\n",
        "\n",
        "    super(NN1Layer, self).__init__()\n",
        "    # input_dim = num of features, Output for binary classification is 1\n",
        "    self.layer_1 = nn.Linear(input_dim, 1)\n",
        "\n",
        "    # output activation function\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  # forward pass\n",
        "  def forward(self, x):\n",
        "    z = self.layer_1(x)\n",
        "    a = self.sigmoid(z)\n",
        "    return a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLXie4eWLSjd"
      },
      "source": [
        "\u2705 **Example: Two neural network layers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLPIVOoIhOhv"
      },
      "source": [
        "\n",
        "<img src=\"https://miro.medium.com/v2/0*GZrkL6Lqt9dIAJ61.jpg\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87V4-6vDFxlM"
      },
      "outputs": [],
      "source": [
        "class NN2Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "\n",
        "        super(NN2Layer, self).__init__()\n",
        "        # input_dim = num of features, hidden_dim = num of neurons\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # hidden_dim = num of neurons, Output for binary classification is 1\n",
        "        self.layer2 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # non-linearity activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        # output activation function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self, x):\n",
        "        z1 = self.layer1(x)\n",
        "        a1 = self.relu(z1)\n",
        "        z2 = self.layer2(a1)\n",
        "        a2 = self.sigmoid(z2)\n",
        "        return a2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slEZs4vBe1dJ"
      },
      "source": [
        "\n",
        "\n",
        "> We can instantiate the model and print it to see the architecture, layers, and total number of trainable parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKrJiWQTdgYZ",
        "outputId": "b64626a0-a679-4d53-aceb-c6ee19f5fce0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Architecture:\n",
            "\n",
            "NN2Layer(\n",
            "  (layer1): Linear(in_features=4, out_features=3, bias=True)\n",
            "  (layer2): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Total trainable parameters: 19\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the model\n",
        "input_dim = 4     # number of input features\n",
        "hidden_dim = 3    # number of hidden neurons\n",
        "\n",
        "model = NN2Layer(input_dim, hidden_dim)\n",
        "\n",
        "# Print the model architecture\n",
        "print(\"Model Architecture:\\n\")\n",
        "print(model)\n",
        "\n",
        "# Calculate the total number of trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nTotal trainable parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naYO9UOyvWYR"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o42xnLsEYr8D"
      },
      "source": [
        "## 3\ufe0f\u20e3 **Training Loop**\n",
        "\n",
        "The **training loop** is responsible for **updating the model's weights** so that it learns to minimize the loss function.\n",
        "\n",
        "### \ud83e\udde9 **Parameters**\n",
        "\n",
        "- **`model`** \u2013 The neural network to be trained.  \n",
        "- **`optimizer`** \u2013 Updates model parameters (e.g., SGD, Adam).  \n",
        "- **`criterion`** \u2013 Loss function, depends on the task.  \n",
        "- **`train_loader`** \u2013 PyTorch `DataLoader` that provides batches of training data.  \n",
        "- **`device`** \u2013 Device used for computation (`cpu` or `cuda`).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfanJzrdnFA_"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, criterion, train_loader, device):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # Move batch to the selected device\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.view(-1, 1).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        # Backward pass & optimization\n",
        "        optimizer.zero_grad()   # Clear previous gradients\n",
        "        loss.backward()         # Compute gradients\n",
        "        optimizer.step()        # Update model parameters\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Average loss over all batches\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FYUwAxlpXzY"
      },
      "source": [
        "###\ud83d\udccc **For criterions (Loss Functions)**:\n",
        "Different tasks require different loss functions\n",
        "- Linear Regression \u2192 `nn.MSELoss()`  \n",
        "- Binary classification \u2192 `nn.BCELoss()`  \n",
        "- Multiclass classification \u2192 `nn.CrossEntropyLoss()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDYXDDeNqwCY"
      },
      "source": [
        "####1\ufe0f\u20e3 `nn.MSELoss()`\n",
        "- `MSELoss` expects **continuous values** (regression).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDWNgCK2sCDo",
        "outputId": "58ed0bf8-be98-4047-bded-ddd70cee0365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE Loss: 0.25\n"
          ]
        }
      ],
      "source": [
        "# Example predictions and targets\n",
        "y_pred = torch.tensor([[2.5], [3.0], [4.5]])\n",
        "y_true = torch.tensor([[3.0], [2.5], [5.0]])\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "loss = criterion(y_pred, y_true)\n",
        "print(\"MSE Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9OMs--Rs4Jk"
      },
      "source": [
        "####2\ufe0f\u20e3 `nn.BCELoss()`\n",
        "- `BCELoss` expects **probabilities** between 0 and 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0lNu1qss7rz",
        "outputId": "89bb69ea-7ef7-46d3-f974-8d9e216fc99d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary Cross Entropy Loss: 0.36354804039001465\n"
          ]
        }
      ],
      "source": [
        "# Predicted probabilities (after sigmoid)\n",
        "y_pred = torch.tensor([[0.8], [0.3], [0.6]])\n",
        "y_true = torch.tensor([[1.0], [0.0], [1.0]])\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "loss = criterion(y_pred, y_true)\n",
        "print(\"Binary Cross Entropy Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr8bgGs6tFJi"
      },
      "source": [
        "####3\ufe0f\u20e3 `nn.CrossEntropyLoss()`\n",
        "- `CrossEntropyLoss` expects **raw logits** (no Softmax needed).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaba4iKbta-I",
        "outputId": "51f05ce3-5771-4eec-c9c3-bb0ba359d951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross Entropy Loss: 0.450598806142807\n"
          ]
        }
      ],
      "source": [
        "# Raw model outputs (logits), shape: (batch_size, num_classes)\n",
        "logits = torch.tensor([\n",
        "    [2.0, 0.5, 1.0],\n",
        "    [0.1, 1.5, 0.3]\n",
        "])\n",
        "# True class indices\n",
        "targets = torch.tensor([0, 1])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = criterion(logits, targets)\n",
        "print(\"Cross Entropy Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UlJgjmKvjbp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Isn-l9gYyfT"
      },
      "source": [
        "\n",
        "## 4\ufe0f\u20e3 **Validation Loop**\n",
        "\n",
        "The **validation loop** evaluates the model\u2019s performance on unseen data **without updating the weights**.\n",
        "\n",
        "It is used to measure how well the model generalizes.\n",
        "\n",
        "### \ud83e\udde9 Parameters\n",
        "\n",
        "- **`model`** \u2013 The trained neural network to be evaluated.  \n",
        "- **`criterion`** \u2013 Loss function used for evaluation\n",
        "- **`test_loader`** \u2013 PyTorch `DataLoader` that provides batches of validation/test data.  \n",
        "- **`device`** \u2013 Device used for computation (`cpu` or `cuda`).  \n",
        "\n",
        "\n",
        "\ud83d\udccc **Note:**  \n",
        "Gradients are disabled during validation using `torch.no_grad()` to improve efficiency and prevent weight updates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxMyr159d5UW"
      },
      "outputs": [],
      "source": [
        "def validate(model, criterion, test_loader, device):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            # Move batch to the selected device\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.view(-1, 1).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Binary predictions\n",
        "            predicted = (outputs > 0.5).float()\n",
        "\n",
        "            # Accuracy calculation\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(test_loader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bcKsHoLvTxk"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkoj068knVPN"
      },
      "source": [
        "## **\ud83d\udccc Full Training Process in PyTorch**\n",
        "\n",
        "Now that you understand the **Dataset Class, Model Class, Training Loop, and Validation Loop**, it's time to put everything together into a **full training process**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUK2taqBkUyR"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zryj9VCzkaHl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOguR6wwkmlr"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT-TaJLL6isg"
      },
      "outputs": [],
      "source": [
        "data = load_breast_cancer()\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# transform to tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test  = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create TensorDatasets for training and testing\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Create Dataloaders to train and test data in batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Y-oPSxlJOw"
      },
      "source": [
        "### Run full training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIgTf7gdlOQZ",
        "outputId": "3ce0afc0-00af-4129-cebe-ea24a62bbe09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Starting Training...\n",
            "Epoch [5/50], Train Loss: 0.5033, Val Loss: 0.4893, Val Accuracy: 0.9035\n",
            "Epoch [10/50], Train Loss: 0.2352, Val Loss: 0.2549, Val Accuracy: 0.9386\n",
            "Epoch [15/50], Train Loss: 0.1410, Val Loss: 0.1643, Val Accuracy: 0.9561\n",
            "Epoch [20/50], Train Loss: 0.1111, Val Loss: 0.1297, Val Accuracy: 0.9474\n",
            "Epoch [25/50], Train Loss: 0.0915, Val Loss: 0.1127, Val Accuracy: 0.9561\n",
            "Epoch [30/50], Train Loss: 0.0765, Val Loss: 0.1032, Val Accuracy: 0.9561\n",
            "Epoch [35/50], Train Loss: 0.0670, Val Loss: 0.0972, Val Accuracy: 0.9649\n",
            "Epoch [40/50], Train Loss: 0.0831, Val Loss: 0.0932, Val Accuracy: 0.9561\n",
            "Epoch [45/50], Train Loss: 0.0577, Val Loss: 0.0909, Val Accuracy: 0.9649\n",
            "Epoch [50/50], Train Loss: 0.0542, Val Loss: 0.0885, Val Accuracy: 0.9649\n",
            "Training Complete!\n"
          ]
        }
      ],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "num_epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Model, Criterion, Optimizer\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 10\n",
        "model =NN2Layer(input_dim, hidden_dim).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Run Training\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "print('Starting Training...')\n",
        "for epoch in range(num_epochs):\n",
        "    # Train one epoch\n",
        "    train_loss = train_one_epoch(model, optimizer, criterion, train_loader, device)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_accuracy = validate(model, criterion, test_loader, device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "print('Training Complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvvMxHD4nfTs"
      },
      "source": [
        "### Plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "1ftDlPPhniA6",
        "outputId": "5875bc86-88ee-4894-ada8-1d7972d6cfa0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting results\n",
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdvSQimxq_3O"
      },
      "source": [
        "## Now you know how to use PyTorch for NNs :)\n",
        "\n",
        "\n",
        "![image.png](https://i.imgur.com/1xbDOQX.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDl5V6-7SOal"
      },
      "source": [
        "### **Contributed by: Yara Alzahrani, Mohamed Eltayeb**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}