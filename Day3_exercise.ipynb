{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_image"
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "#  **Neural Networks for Regression: Diamond Price Prediction** ðŸ’Ž\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_cell"
      },
      "source": [
        "In this exercise, you will:\n",
        "- Predict **diamond prices** using a neural network built with PyTorch\n",
        "- Build a **Three-layer neural network regressor**\n",
        "- Train the model on tabular diamond data\n",
        "- Evaluate the model's performance using **multiple metrics**\n",
        "\n",
        "---\n",
        "\n",
        "## **Tasks Overview**\n",
        "\n",
        "| Task | Description |\n",
        "|------|-------------|\n",
        "| **Task 1** | Load & Explore Data |\n",
        "| **Task 2** | Data Preprocessing (Encoding & Scaling) |\n",
        "| **Task 3** | Create PyTorch DataLoaders |\n",
        "| **Task 4** | Build the Neural Network Model |\n",
        "| **Task 5** | Define Training & Validation Functions |\n",
        "| **Task 6** | Train, Evaluate & Visualize Results |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports_cell"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "about_dataset"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 1: Load & Explore Data\n",
        "\n",
        "## ðŸ“Š **About The Dataset**\n",
        "\n",
        "This is the classic **Diamonds dataset** used for **regression tasks**.  \n",
        "It contains features like **carat, cut, color, clarity, depth, table**, and physical dimensions (**x, y, z**).\n",
        "\n",
        "The **target variable** is **price**, a continuous value we want to predict.\n",
        "\n",
        "Dataset link: https://www.kaggle.com/datasets/natedir/diamonds/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "read_data_header"
      },
      "source": [
        "### ðŸ”¹**Read Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_data"
      },
      "outputs": [],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"natedir/diamonds\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "df = pd.read_csv(path + \"/diamonds.csv\")\n",
        "df = df.drop('Unnamed: 0', axis=1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_info"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prepare_data_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 2: Data Preprocessing\n",
        "\n",
        "### ðŸ”¹**Prepare Data**\n",
        "\n",
        "> Before training the model, we'll prepare the data by **encoding categorical features** and **scaling numerical features** to ensure stable and efficient neural network training.\n",
        "\n",
        "**Preprocessing Checklist:**\n",
        "\n",
        "| Step | Action |\n",
        "|------|--------|\n",
        "| 1 | Encode categorical columns (cut, color, clarity) |\n",
        "| 2 | Scale numerical features (but NOT the target!) |\n",
        "| 3 | Split into features (X) and target (y) |\n",
        "| 4 | Convert to PyTorch tensors |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "encode_data"
      },
      "outputs": [],
      "source": [
        "# TODO: Encode categorical features using LabelEncoder\n",
        "# [HINT]: Use select_dtypes to find categorical columns, then loop through them\n",
        "\n",
        "categorical_cols = None  # <Replace None with your code>\n",
        "\n",
        "for col in categorical_cols:\n",
        "  print(f\"Encoding column: {col}\")\n",
        "  le = LabelEncoder()\n",
        "  # TODO: Apply fit_transform to encode the column\n",
        "  df[col] = None  # <Replace None with your code>\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scale_data"
      },
      "outputs": [],
      "source": [
        "# TODO: Standardize features using StandardScaler (DON'T scale the target!)\n",
        "# [HINT]: Select numerical columns, but drop \"price\" from the list\n",
        "\n",
        "numerical_cols = None  # <Replace None with your code>\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# TODO: Apply fit_transform to scale the numerical columns\n",
        "df[numerical_cols] = None  # <Replace None with your code>\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "split_note"
      },
      "source": [
        "> After preprocessing data, we'll first split the dataset into features and target labels, apply a trainâ€“test split, and then convert the data into PyTorch tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "split_data"
      },
      "outputs": [],
      "source": [
        "# TODO: Split features from target\n",
        "X = None  # <Replace None with your code>\n",
        "y = None  # <Replace None with your code>\n",
        "\n",
        "# TODO: Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = None  # <Replace None with your code>\n",
        "\n",
        "# TODO: Transform to PyTorch tensors with dtype=torch.float32\n",
        "X_train = None  # <Replace None with your code>\n",
        "X_test  = None  # <Replace None with your code>\n",
        "y_train = None  # <Replace None with your code>\n",
        "y_test  = None  # <Replace None with your code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataloader_note"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 3: Create PyTorch DataLoaders\n",
        "\n",
        "> After converting the data into tensors, we group features and labels into **TensorDataset** objects.  \n",
        "We then use **DataLoaders** to load the data in mini-batches for efficient training and evaluation.\n",
        "\n",
        "**Why use DataLoaders?**\n",
        "- Handles **batching** automatically\n",
        "- Enables **shuffling** of training data\n",
        "- Makes training loops cleaner and more efficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataloaders"
      },
      "outputs": [],
      "source": [
        "# TODO: Create TensorDatasets for training and testing\n",
        "# [HINT]: Use TensorDataset(features, labels)\n",
        "train_dataset = None  # <Replace None with your code>\n",
        "test_dataset = None   # <Replace None with your code>\n",
        "\n",
        "# TODO: Create DataLoaders with batch_size=32\n",
        "# [HINT]: shuffle=True for training, shuffle=False for testing\n",
        "train_loader = None  # <Replace None with your code>\n",
        "test_loader = None   # <Replace None with your code>\n",
        "\n",
        "# Print dataset sizes\n",
        "print(\"Train dataset:\", len(train_dataset))\n",
        "print(\"Test dataset:\", len(test_dataset))\n",
        "\n",
        "# Get the first batch from the training DataLoader\n",
        "X_batch, y_batch = next(iter(train_loader))\n",
        "print(f\"Training batch input shape: {X_batch.shape}\")\n",
        "print(f\"Training batch labels shape: {y_batch.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_ready"
      },
      "source": [
        "> Data is now ready for the model! Let's build the model class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 4: Build the Neural Network Model\n",
        "\n",
        "### ðŸ”¹**Model Class**\n",
        "\n",
        "Let's create the **architecture of our model** by implementing a **three-layer neural network regressor**.\n",
        "\n",
        "For regression, the key differences from classification are:\n",
        "- **Output layer** produces a single continuous value (not class probabilities)\n",
        "- **No softmax** is needed at the output\n",
        "- We use **MSE (Mean Squared Error)** loss instead of Cross-Entropy\n",
        "\n",
        "**Architecture Overview:**\n",
        "```\n",
        "Input (9 features) â†’ Linear â†’ ReLU â†’ Linear â†’ ReLU â†’ Linear â†’ Output (1 value)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_class"
      },
      "outputs": [],
      "source": [
        "class NN3Layer(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim):\n",
        "    super(NN3Layer, self).__init__()\n",
        "    # TODO: Define the first linear layer: input_dim -> hidden_dim\n",
        "    self.layer1 = None  # <Replace None with your code>\n",
        "\n",
        "    # TODO: Define the second linear layer: hidden_dim -> hidden_dim\n",
        "    self.layer2 = None  # <Replace None with your code>\n",
        "\n",
        "    # TODO: Define the output layer: hidden_dim -> 1 (single value for regression)\n",
        "    self.layer3 = None  # <Replace None with your code>\n",
        "\n",
        "    # TODO: Define ReLU activation\n",
        "    self.relu = None  # <Replace None with your code>\n",
        "\n",
        "  def forward(self, x):\n",
        "    # TODO: First hidden layer with ReLU\n",
        "    a1 = None  # <Replace None with your code>\n",
        "\n",
        "    # TODO: Second hidden layer with ReLU\n",
        "    a2 = None  # <Replace None with your code>\n",
        "\n",
        "    # TODO: Output layer (no activation for regression)\n",
        "    output = None  # <Replace None with your code>\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 5: Define Training & Validation Functions\n",
        "\n",
        "### ðŸ”¹**Training Loop**\n",
        "\n",
        "> The training loop performs **forward pass**, computes **loss**, does **backpropagation**, and **updates weights**.\n",
        "\n",
        "**Training Steps per Batch:**\n",
        "1. Move data to device (GPU)\n",
        "2. Forward pass: Get model predictions\n",
        "3. Compute loss (MSE for regression)\n",
        "4. Zero gradients, Backward pass, Update weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_loop"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, criterion, train_loader, device):\n",
        "  # TODO: Set the model to training mode\n",
        "  # <YOUR CODE HERE>\n",
        "\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for X_batch, y_batch in train_loader:\n",
        "    # TODO: Move batch to the selected device\n",
        "    X_batch = None  # <Replace None with your code>\n",
        "    y_batch = None  # <Replace None with your code> [HINT: reshape to (-1, 1)]\n",
        "\n",
        "    # TODO: Forward pass - get model predictions\n",
        "    outputs = None  # <Replace None with your code>\n",
        "\n",
        "    # TODO: Compute loss using criterion\n",
        "    loss = None  # <Replace None with your code>\n",
        "\n",
        "    # TODO: Backward pass & optimization\n",
        "    # Step 1: Clear previous gradients\n",
        "    # <YOUR CODE HERE>\n",
        "\n",
        "    # Step 2: Compute gradients (backward pass)\n",
        "    # <YOUR CODE HERE>\n",
        "\n",
        "    # Step 3: Update model parameters\n",
        "    # <YOUR CODE HERE>\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  # Calculate average loss over all batches\n",
        "  avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "  return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validation_header"
      },
      "source": [
        "### ðŸ”¹**Validation Loop**\n",
        "\n",
        "> The validation loop is similar to training, but with key differences:\n",
        "> - Model is set to **evaluation mode** (`model.eval()`)\n",
        "> - We use `torch.no_grad()` to **disable gradient computation**\n",
        "> - No weight updates happen, we just evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validation_loop"
      },
      "outputs": [],
      "source": [
        "def validate(model, criterion, test_loader, device):\n",
        "  # TODO: Set the model to evaluation mode\n",
        "  # <YOUR CODE HERE>\n",
        "\n",
        "  running_loss = 0.0\n",
        "\n",
        "  # TODO: Disable gradient computation using torch.no_grad()\n",
        "  with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "      # TODO: Move data to device\n",
        "      X_batch = None  # <Replace None with your code>\n",
        "      y_batch = None  # <Replace None with your code> [HINT: reshape to (-1, 1)]\n",
        "\n",
        "      # TODO: Forward pass - get model predictions\n",
        "      outputs = None  # <Replace None with your code>\n",
        "\n",
        "      # TODO: Compute loss using criterion\n",
        "      loss = None  # <Replace None with your code>\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "  avg_loss = running_loss / len(test_loader)\n",
        "\n",
        "  return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "running_training_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 6: Train and Evaluate the Model\n",
        "\n",
        "### ðŸ”¹**Model Setup**\n",
        "\n",
        "> Now let's put everything together! We'll:\n",
        "> 1. Set up the device (GPU if available)\n",
        "> 2. Instantiate our model\n",
        "> 3. Define the loss function and optimizer\n",
        "> 4. Run the training loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Set up the device (use GPU if available, otherwise CPU)\n",
        "device = None  # <Replace None with your code>"
      ],
      "metadata": {
        "id": "oLaCZCuR1NaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_setup"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "input_dim = X_train.shape[1]   # Number of tabular features\n",
        "hidden_dim = 64                # Design choice (feel free to experiment!)\n",
        "\n",
        "# TODO: Instantiate the model and move it to the device\n",
        "model = None  # <Replace None with your code>\n",
        "\n",
        "# Print the model architecture\n",
        "print(\"Model Architecture:\\n\")\n",
        "print(model)\n",
        "\n",
        "# Calculate the total number of trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal trainable parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_setup"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters (feel free to experiment!)\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define criterion (loss function) - use MSELoss for regression\n",
        "criterion = None  # <Replace None with your code>\n",
        "\n",
        "# TODO: Define optimizer - use AdamW with the model parameters and learning rate\n",
        "optimizer = None  # <Replace None with your code>"
      ],
      "metadata": {
        "id": "8ciNmhde1jXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF-PCwEH3UCz"
      },
      "source": [
        "### ðŸ”¹**Run Training**\n",
        "\n",
        "> Now we're ready to train! The training loop will:\n",
        "> - Train for the specified number of epochs\n",
        "> - Track both training and validation loss\n",
        "> - Print progress after each epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Run Training\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print('Starting Training...')\n",
        "for epoch in range(num_epochs):\n",
        "  # TODO: Train one epoch using the train_one_epoch function\n",
        "  train_loss = None  # <Replace None with your code>\n",
        "\n",
        "  # TODO: Validate using the validate function\n",
        "  val_loss = None  # <Replace None with your code>\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "print('Training Complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plot_note"
      },
      "source": [
        "### ðŸ”¹**Visualize Results**\n",
        "\n",
        "> Let's plot the training and validation losses to see how our model learned over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_losses"
      },
      "outputs": [],
      "source": [
        "# Plotting results\n",
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FUb4Kqf3UCz"
      },
      "source": [
        "### ðŸ“Š **Evaluate with More Metrics**\n",
        "\n",
        "> Beyond MSE loss, we can evaluate our regression model using additional metrics:\n",
        "> - **MAE (Mean Absolute Error)**: Average absolute difference between predictions and actual values\n",
        "> - **RMSE (Root Mean Squared Error)**: Square root of MSE, in the same units as the target\n",
        "> - **RÂ² Score**: Proportion of variance explained by the model (1.0 = perfect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGIWupJ33UCz"
      },
      "outputs": [],
      "source": [
        "# Get predictions on test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  # TODO: Get predictions for X_test\n",
        "  # [HINT]: Move X_test to device, pass through model, then move back to CPU\n",
        "  predictions = None  # <Replace None with your code>\n",
        "\n",
        "# TODO: Calculate evaluation metrics\n",
        "# [HINT]: Use sklearn metrics - mean_absolute_error, mean_squared_error, r2_score\n",
        "mae = None   # <Replace None with your code>\n",
        "mse = None   # <Replace None with your code>\n",
        "rmse = None  # <Replace None with your code> [HINT: use np.sqrt()]\n",
        "r2 = None    # <Replace None with your code>\n",
        "\n",
        "print(\"Model Evaluation\")\n",
        "print(f\"  MAE:  ${mae:.2f}\")\n",
        "print(f\"  MSE:  {mse:.2f}\")\n",
        "print(f\"  RMSE: ${rmse:.2f}\")\n",
        "print(f\"  R2:   {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEiCMhqK3UCz"
      },
      "source": [
        "### ðŸ“ˆ **Visualize: Predicted vs Actual**\n",
        "\n",
        "> Let's visualize how well our model's predictions match the actual diamond prices.\n",
        "> - A **perfect model** would have all points on the diagonal line\n",
        "> - Points **above the line** = model underestimated the price\n",
        "> - Points **below the line** = model overestimated the price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgSPEJyI3UCz"
      },
      "outputs": [],
      "source": [
        "# Scatter plot: Predicted vs Actual\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot: Predicted vs Actual scatter\n",
        "plt.scatter(y_test.numpy(), predictions.flatten(), alpha=0.5, s=10, c='steelblue')\n",
        "\n",
        "# Add perfect prediction line\n",
        "min_val = min(y_test.min(), predictions.min())\n",
        "max_val = max(y_test.max(), predictions.max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
        "\n",
        "plt.xlabel('Actual Price ($)', fontsize=12)\n",
        "plt.ylabel('Predicted Price ($)', fontsize=12)\n",
        "plt.title('Predicted vs Actual Diamond Prices', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6FOY6vf3UDA"
      },
      "source": [
        "### **Contribution: Yara Alzahrani & Sattam Altwaim** :)\n",
        "\n",
        "![image.jpeg](https://i.imgur.com/Q4egy5l.jpeg)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}